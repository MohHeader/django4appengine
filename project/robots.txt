# See http://www.robotstxt.org/wc/norobots.html for documentation on how to use the robots.txt file

#special spiders needs to be allowed
User-agent: Googlebot
Disallow: 
 
User-agent: Bingbot
Disallow: 

User-agent: googlebot-image
Disallow: 
 
User-agent: googlebot-mobile
Disallow: 
 
User-agent: MSNBot
Disallow: 
 
User-agent: ia_archiver
Disallow: 
 
User-agent: yahoo-mmcrawler
Disallow: 
 
User-agent: psbot
Disallow: 

User-agent: yahoo-blogs/v3.9
Disallow: 
 
#Yahoo
User-agent: Slurp
Disallow: 

User-agent: Yandex
Disallow: 

#Seomoz
User-agent: rogerbot
Disallow: 
 
User-agent: YahooSeeker/M1A1-R2D2
Disallow: 
 
User-agent: Teoma
Disallow: 

User-agent: FacebookExternalHit/*
Disallow: 

#google ads
User-agent: Mediapartners-Google 
Disallow: 

# To ban all spiders except the above from the entire site:
User-Agent: *
Allow: /robots.txt
Disallow: /

#Sitemap: http://aap-id.appspot.com/sitemap.xml
